{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "sys.path.append('../')\n",
    "from models.cnn.search_cnn import  SearchCNN, SearchCNNController\n",
    "from configobj import ConfigObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basecfg_path = '../configs/mnist/darts.cfg'  #конфиг, на который мы ориентируемся при загрузки модели\n",
    "cfg = ConfigObj(basecfg_path)\n",
    "name = cfg['name'] # имя для сохранения результатов\n",
    "ckp_path = './var_nas/searchs/'+name+'/best_{}.pth.tar' # это шаблон названия сохраненных моделей\n",
    "seeds = cfg['seeds'].split(';')  # сиды. можно брать из конфига\n",
    "cfg['device'] = 'cpu'\n",
    "sc = SearchCNNController(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = torch.distributions.categorical.Categorical(logits=sc.alpha_reduce[0])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1428, 0.1429, 0.1428, 0.1429, 0.1429, 0.1428, 0.1429],\n",
       "        [0.1428, 0.1427, 0.1428, 0.1428, 0.1429, 0.1431, 0.1430]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distr.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1428, 0.1429, 0.1428, 0.1429, 0.1429, 0.1428, 0.1429],\n",
       "        [0.1428, 0.1427, 0.1428, 0.1428, 0.1429, 0.1431, 0.1430]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distr = torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(logits=sc.alpha_reduce[0], temperature=0.1) \n",
    "distr.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(model):\n",
    "    entropy = 0\n",
    "    for alpha in model.alpha_reduce:\n",
    "        for subalpha in alpha:\n",
    "            distr = torch.distributions.categorical.Categorical(probs=subalpha)            \n",
    "            entropy += distr.entropy()\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0349, grad_fn=<AddBackward0>)\n",
      "tensor(1.0349, grad_fn=<AddBackward0>)\n",
      "tensor(1.0349, grad_fn=<AddBackward0>)\n",
      "tensor(1.0349, grad_fn=<AddBackward0>)\n",
      "tensor(1.0349, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# посмотрим, какова будет энтропия у моделей с низкой температурой и концентрацией плотности у одной из операций\n",
    "# здесь уже не надо загружать сохраненные параметры. Просто смотрим энтропию, на которую надо ориентироваться\n",
    "torch.manual_seed(0) # чтобы результат не менялся\n",
    "for _ in seeds:    \n",
    "    for alpha in sc.alpha_reduce:\n",
    "        for subalpha in alpha:\n",
    "            subalpha.data *= 0\n",
    "            # подобрал на глаз. Вероятность первой компоненты колеблется от 0.8 до 0.98\n",
    "            subalpha.data[0] += 0.99 \n",
    "            subalpha.data[1:] += 0.01/(subalpha.data[1:].shape[0])            \n",
    "    print (calc_entropy(sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.2428, grad_fn=<AddBackward0>)\n",
      "tensor(27.2428, grad_fn=<AddBackward0>)\n",
      "tensor(27.2428, grad_fn=<AddBackward0>)\n",
      "tensor(27.2428, grad_fn=<AddBackward0>)\n",
      "tensor(27.2428, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# посмотрим, какова будет энтропия у моделей с низкой температурой и концентрацией плотности у одной из операций\n",
    "# здесь уже не надо загружать сохраненные параметры. Просто смотрим энтропию, на которую надо ориентироваться\n",
    "torch.manual_seed(0) # чтобы результат не менялся\n",
    "for _ in seeds:    \n",
    "    for alpha in sc.alpha_reduce:\n",
    "        for subalpha in alpha:\n",
    "            subalpha.data *= 0\n",
    "            # подобрал на глаз. Вероятность первой компоненты колеблется от 0.8 до 0.98   \n",
    "            subalpha.data += 1/(subalpha.data.shape[0])            \n",
    "    print (calc_entropy(sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим, какова будет энтропия у моделей с единичной температурой и концентрацией плотности в центре симплекса\n",
    "# здесь уже не надо загружать сохраненные параметры. Просто смотрим энтропию, на которую надо ориентироваться\n",
    "torch.manual_seed(0) # чтобы результат не менялся\n",
    "for _ in seeds:    \n",
    "    for alpha in sc.alpha_reduce:\n",
    "        for subalpha in alpha:\n",
    "            subalpha.data *= 0\n",
    "            # подобрал на глаз. Вероятность каждой компоненты колеблется от 0.11 о 0.17\n",
    "            subalpha.data += torch.randn(subalpha.shape)*0.1\n",
    "    print (calc_entropy(sc, 0.2))\n",
    "    # NB: если делать высокую температуру, то энтропия также уменьшается:\n",
    "    # сэмлпы будут концентрироваться ближе к симплексу"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Итог: для сильно фиксированных структур энтропия колеблется в районе -2000,-1700\n",
    "для максимально энтропийных структур: около -110,-120\n",
    "среднее значение можно брать около -500,-600\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basecfg_path = '../configs/mnist/darts_test_igr.cfg'  #конфиг, на который мы ориентируемся при загрузки модели\n",
    "cfg = ConfigObj(basecfg_path)\n",
    "name = cfg['name'] # имя для сохранения результатов\n",
    "ckp_path = './var_nas/searchs/'+name+'/best_{}.pth.tar' # это шаблон названия сохраненных моделей\n",
    "seeds = cfg['seeds'].split(';')  # сиды. можно брать из конфига\n",
    "cfg['device'] = 'cpu'\n",
    "sc = SearchCNNController(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://arxiv.org/pdf/1912.09588.pdf\n",
    "def log_det_jac(logits, t):\n",
    "    k = len(logits)    \n",
    "    exp_logit = torch.exp(logits/t)\n",
    "    eps = 0.01\n",
    "    part1 = -2*(k-1) * torch.log((exp_logit.sum()))\n",
    "    \n",
    "    part2 = torch.log(abs(1-t*(exp_logit/(eps+logits)).sum()))\n",
    "    \n",
    "    part3 = -(k-1) * torch.log(t)\n",
    "    \n",
    "    part4 = (torch.log(abs(logits))+abs(logits/t)).sum() \n",
    "    \n",
    "    return part1 + part2 + part3 +part4\n",
    "log_det_jac(torch.randn(5), torch.ones(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(model, temp, entropy_sample_num = 100):\n",
    "    entropy = 0\n",
    "    for alpha, cov in zip(model.alpha_reduce, model.alpha_cov_reduce):\n",
    "        for subalpha, subcov in zip(alpha, cov):\n",
    "            distr = torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(subalpha,\n",
    "                                                                                              subcov,\n",
    "                                                                                              torch.ones(subalpha.shape[0]).to('cpu'))\n",
    "            subentropy = 0\n",
    "            for _ in range(entropy_sample_num):\n",
    "                sample = distr.sample()                \n",
    "                subentropy=subentropy -distr.log_prob(sample) +log_det_jac(sample, torch.ones(1)*temp)\n",
    "\n",
    "            subentropy /= entropy_sample_num\n",
    "            entropy += subentropy\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим, какова будет энтропия у моделей с низкой температурой и концентрацией плотности у одной из операций\n",
    "# здесь уже не надо загружать сохраненные параметры. Просто смотрим энтропию, на которую надо ориентироваться\n",
    "torch.manual_seed(0) # чтобы результат не менялся\n",
    "for _ in seeds:    \n",
    "    for alpha, cov in zip(sc.alpha_reduce, sc.alpha_cov_reduce):\n",
    "        for subalpha, subcov in zip(alpha, cov):\n",
    "            subalpha.data *= 0            \n",
    "            # подобрал на глаз. Вероятность первой компоненты колеблется от 0.8 до 0.98\n",
    "            subalpha.data += torch.randn(subalpha.shape)\n",
    "            \n",
    "            subalpha.data *= 0            \n",
    "            # подобрал на глаз. Вероятность первой компоненты колеблется от 0.8 до 0.98\n",
    "            subalpha.data += torch.randn(subalpha.shape)\n",
    "            \n",
    "            subcov.data *= 0\n",
    "            subcov.data += torch.randn(subalpha.shape)*0.1\n",
    "            subalpha.data[0] += 5 \n",
    "    print (calc_entropy(sc, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим, какова будет энтропия у моделей с единичной температурой и концентрацией плотности в центре симплекса\n",
    "# здесь уже не надо загружать сохраненные параметры. Просто смотрим энтропию, на которую надо ориентироваться\n",
    "torch.manual_seed(0) # чтобы результат не менялся\n",
    "for _ in seeds:    \n",
    "    for alpha, cov in zip(sc.alpha_reduce, sc.alpha_cov_reduce):\n",
    "        for subalpha, subcov in zip(alpha, cov):\n",
    "            subalpha.data *= 0\n",
    "            # подобрал на глаз. Вероятность каждой компоненты колеблется от 0.11 о 0.17\n",
    "            subalpha.data += torch.randn(subalpha.shape)*0.1\n",
    "            \n",
    "            subcov.data *= 0\n",
    "            subcov.data += torch.randn(subalpha.shape)*0.1\n",
    "    print (calc_entropy(sc, 1))\n",
    "    # NB: если делать высокую температуру, то энтропия также уменьшается:\n",
    "    # сэмлпы будут концентрироваться ближе к симплексу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Итог: для сильно фиксированных структур энтропия колеблется в районе -3000,-2500\n",
    "для максимально энтропийных структур: около -200,-150\n",
    "среднее значение можно брать около -1000,-700\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
